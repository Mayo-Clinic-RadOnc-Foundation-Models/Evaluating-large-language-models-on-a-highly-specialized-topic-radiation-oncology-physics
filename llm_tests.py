# -*- coding: utf-8 -*-
"""LLM-Tests.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wTHBHd-JJl5Z7lj7ycBVb1n8aaA4Ex8z
"""

import random
from collections import Counter
import re
import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np
import string
import time
import os
import matplotlib.pyplot as plt
import pandas as pd
import openai
from google.cloud import aiplatform
from google.cloud.aiplatform.gapic.schema import predict
from google.protobuf import json_format
from google.protobuf.struct_pb2 import Value
import json
from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT
import sensenova

def gpt_one_shot(system_prompt, user_prompt, print_response=True, model="gpt-4", retries=5, temperature=0.0):
    for i in range(retries):
        try:
            # settings
            model = model

            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "system", "content": f"{system_prompt}"},{"role": "user", "content": f"{user_prompt}"}],
                temperature=temperature,
                stream=True
            )

            complete_response = ''
            for chunk in response:
                if 'choices' in chunk:
                    for choice in chunk['choices']:
                        if 'delta' in choice and 'content' in choice['delta']:
                            chunk_content = choice['delta']['content']
                            if print_response:
                                print(chunk_content, end='')
                            complete_response += chunk_content
            if print_response:
                print('\n', end='')

            return complete_response

        except Exception as e:
            if i < retries - 1:  # i is zero indexed
                time.sleep(i)  # wait a bit before trying again
                print('Request failed:\n\tsystem prompt:\n\t{}\n\tuser prompt:\n\t{}\n\ntrying again (attempt: {}) ...'.format(system_prompt, user_prompt, i))
                continue
            else:
                # If you've retried retries times and still have an error, you can optionally log or print it out here
                print(f"Request failed after {retries} tries with error: {e}")
                raise

def palm_one_shot(system_prompt, user_prompt, print_response=True, retries=5, temperature=0.0):
    # Before this function will work, you must:
    #
    # (1) open the "Google Cloud SDK Shell".
    # (2) Enter $ gcloud auth application-default login
    # (3) Log in (you'll be prompted to login)

    for i in range(retries):
        try:
            # client options
            client_options = {"api_endpoint": ""}
            client = aiplatform.gapic.PredictionServiceClient(
                client_options=client_options
            )

            # the model endpoint
            endpoint = ""

            # settings
            max_output_tokens = 1024

            # initialize messages
            messages = []

            # append the user's message to the conversation
            messages.append({
                "author": "user",
                "content": system_prompt + '\n\n' + user_prompt
            })

            # create an instance for prediction
            instance_dict = {"messages": messages}
            instance = json_format.ParseDict(instance_dict, Value())
            instances = [instance]

            # set the parameters
            parameters_dict = {
                "temperature": temperature,
                "maxOutputTokens": max_output_tokens
            }
            parameters = json_format.ParseDict(parameters_dict, Value())

            # make a prediction
            response = client.predict(
                endpoint=endpoint,
                instances=instances,
                parameters=parameters
            )

            # get the bot's reply
            bot_reply = response.predictions[0]["candidates"][0]["content"]
            if print_response:
                print(bot_reply)

            return bot_reply

        except Exception as e:
            if i < retries - 1:  # i is zero indexed
                time.sleep(i)  # wait a bit before trying again
                print('Request failed, trying again (attempt: {}) ...'.format(i))
                continue
            else:
                # If you've retried retries times and still have an error, you can optionally log or print it out here
                print(f"Request failed after {retries} tries with error: {e}")
                raise

def claude_one_shot(system_prompt, user_prompt, print_response=True, retries=5, temperature=0.0):
    for i in range(retries):
        try:
            prompt = '\n\nHuman: ' + system_prompt + '\n\n' + user_prompt + '\n\nAssistant:'

            anthropic = Anthropic(
                api_key=os.environ.get("ANTHROPIC_API_KEY"),
            )
            completion = anthropic.completions.create(
                model="claude-2",
                max_tokens_to_sample=512,
                temperature=temperature,
                # top_k=40,
                # top_p=0.9,
                prompt=prompt
            )
            if print_response:
                print(completion.completion)

            return completion.completion

        except Exception as e:
            if i < retries - 1:  # i is zero indexed
                time.sleep(i)  # wait a bit before trying again
                print('Request failed, trying again (attempt: {}) ...'.format(i))
                continue
            else:
                # If you've retried retries times and still have an error, you can optionally log or print it out here
                print(f"Request failed after {retries} tries with error: {e}")
                raise

def sensenova_one_shot(system_prompt, user_prompt, print_response=True, retries=5, temperature=0.0):
    for i in range(retries):
        try:
            prompt = system_prompt + '\n\n' + user_prompt

            resp = sensenova.ChatCompletion.create(
                messages=[{"role": "user", "content": prompt}],
                model="nova-ptc-xl-v1",
                stream=False,
                tempertature=temperature, # 'tempertature' is spelled wrong in API :(
                # topk=40,
                # topp=0.9,
                access_key_id='',
                secret_access_key=''
            )
            message = resp.to_dict_recursive()['data']['choices'][0]['message']

            if print_response:
                print(message)

            return message

        except Exception as e:
            if i < retries - 1:  # i is zero indexed
                time.sleep(i)  # wait a bit before trying again
                print('Request failed, trying again (attempt: {}) ...'.format(i))
                continue
            else:
                # If you've retried retries times and still have an error, you can optionally log or print it out here
                print(f"Request failed after {retries} tries with error: {e}")
                raise

def process_files(dir_path):
    # Initialize an empty dictionary to store tests
    test_dict = {}

    # Iterate over every file in the directory
    filenames = []
    for filename in os.listdir(dir_path):
        # Make sure we're only processing .txt files
        if filename.endswith(".txt"):
            filenames.append(filename)
            # Create a full file path by joining the directory path and the filename
            full_path = os.path.join(dir_path, filename)

            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
            except:
                with open(full_path, 'r', encoding='ISO-8859-1') as f:
                    lines = f.readlines()

            # Separate content from comments and answers
            content = []
            answers = []
            comments = []

            for i, line in enumerate(lines):
                line = line.strip()  # Remove trailing and leading whitespaces
                if line.startswith('#'):
                    if i == 0:  # if it's the first line, it contains the answers
                        answers = list(line[1:].replace(',', '').strip())
                    elif i == 1:  # if it's the second line, it contains the categories
                        categories_raw = line[1:].strip().split(', ')
                        categories = {}
                        for cat in categories_raw:
                            name, numbers = [x.strip() for x in cat.split(' (')]
                            numbers = [x.strip().replace(')', '') for x in numbers.split('+')]
                            ranges = []
                            for num in numbers:
                                # Extract all numbers from the string using regex
                                num_parts = re.findall(r'\d+', num)

                                if len(num_parts) == 2:
                                    start, end = map(int, num_parts)
                                    ranges.append((start, end))
                                elif len(num_parts) == 1:
                                    single_num = int(num_parts[0])
                                    ranges.append((single_num, single_num))
                            categories[name] = ranges
                        comments.append(categories)
                    else:
                        comments.append(line[1:])
                else:
                    content.append(line)

            # Initialize variables for questions and expected next number
            questions = []
            next_num = 1

            # The regex pattern looks for one or more digits (\d+) at the start (^) of a line
            # followed by a period (\.). The parentheses are used to capture the pattern.
            pattern = r'(\d+)'
            parts = re.split(pattern, '\n'.join(content))

            # Iterate over the parts. Each odd-indexed part should be a number and period,
            # and each even-indexed part should be a question.
            for i in range(1, len(parts), 2):
                # Convert the number part to an integer
                num = int(parts[i].strip('.'))

                # Check that the number is the expected next number
                if num == next_num:
                    # If the number is correct, add the question to the list
                    questions.append(parts[i] + parts[i+1])
                    # And increment the expected next number
                    next_num += 1
                    # print('Question {} was processed.'.format(num))
                else:
                    # If the number is not correct, add this part to the last question in the list
                    if questions:
                        questions[-1] += parts[i] + parts[i+1]

            # Add the questions, answers, comments, and categories to the dictionary
            test_dict[filename] = {'questions': questions, 'answers': answers, 'comments': comments[1:], 'categories': comments[0]}

    return test_dict, filenames

def most_common_answer(answer_list):
    answer_count = Counter(answer_list)
    max_count = max(answer_count.values())
    most_common_answers = [answer for answer, count in answer_count.items() if count == max_count]
    return random.choice(most_common_answers)

def numpy_to_list(obj):
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: numpy_to_list(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [numpy_to_list(x) for x in obj]
    else:
        return obj

def do_test(instructions_prompts, questions_list, correct_answers_list, temperature=0, samples_per_prompt=5):
    # description:

    answers_gpt4 = []
    answers_gpt3 = []
    answers_palm2 = []
    answers_claude2 = []
    answers_sensenova = []

    for i, question in enumerate(questions_list):
        answers_gpt4_this_question = []
        answers_gpt3_this_question = []
        answers_palm2_this_question = []
        answers_claude2_this_question = []
        answers_sensenova_this_question = []

        for instructions_prompt in instructions_prompts:

            answers_gpt4_this_prompt = []
            answers_gpt3_this_prompt = []
            answers_palm2_this_prompt = []
            answers_claude2_this_prompt = []
            answers_sensenova_this_prompt = []

            grading_prompt = "I need help to interpret the response to a question. Given the following question and response (denoted by '## QUESTION ##' and '## RESPONSE ##'), determine the letter-choice reflected by the response. Do not justify your answer. For example, if the response suggested the answer was letter-choice A, then return the following:\nA\n\nIf the response suggested the answer was C, then return:\nC\n\nIf the response doesn't make sense, return the following:\nZ\n\nIn any case, only return a single letter choice as the answer."

            for j in np.arange(samples_per_prompt):
                # gpt-4
                response_raw = gpt_one_shot(instructions_prompt, question, print_response=False, model="gpt-4", retries=100, temperature=temperature)
                response_formatted = '## QUESTION ##\n' + question + '\n## RESPONSE ##\n' + response_raw
                response_answer = gpt_one_shot(grading_prompt, response_formatted, print_response=False, model="gpt-4", retries=100, temperature=0)
                answers_gpt4_this_prompt.append(response_answer)

                # gpt-3.5
                response_raw = gpt_one_shot(instructions_prompt, question, print_response=False, model="gpt-3.5-turbo", retries=100, temperature=temperature)
                response_formatted = '## QUESTION ##\n' + question + '\n## RESPONSE ##\n' + response_raw
                response_answer = gpt_one_shot(grading_prompt, response_formatted, print_response=False, model="gpt-4", retries=100, temperature=0)
                answers_gpt3_this_prompt.append(response_answer)

                # palm 2
                response_raw = palm_one_shot(instructions_prompt, question, print_response=False, retries=100, temperature=temperature)
                response_formatted = '## QUESTION ##\n' + question + '\n## RESPONSE ##\n' + response_raw
                response_answer = gpt_one_shot(grading_prompt, response_formatted, print_response=False, model="gpt-4", retries=100, temperature=0)
                answers_palm2_this_prompt.append(response_answer)

                # sensenova
                response_raw = sensenova_one_shot(instructions_prompt, question, print_response=False, retries=100, temperature=temperature)
                response_formatted = '## QUESTION ##\n' + question + '\n## RESPONSE ##\n' + response_raw
                response_answer = gpt_one_shot(grading_prompt, response_formatted, print_response=False, model="gpt-4", retries=100, temperature=0)
                answers_sensenova_this_prompt.append(response_answer)

                # claude 2
                response_raw = claude_one_shot(instructions_prompt, question, print_response=False, retries=100, temperature=temperature)
                response_formatted = '## QUESTION ##\n' + question + '\n## RESPONSE ##\n' + response_raw
                response_answer = gpt_one_shot(grading_prompt, response_formatted, print_response=False, model="gpt-4", retries=100, temperature=0)
                answers_claude2_this_prompt.append(response_answer)

            print(response_formatted)
            print('answer given (determined by GPT-4):', response_answer)

            answer_gpt4_this_prompt = most_common_answer(answers_gpt4_this_prompt)
            answers_gpt4_this_question.append(answer_gpt4_this_prompt)

            answer_gpt3_this_prompt = most_common_answer(answers_gpt3_this_prompt)
            answers_gpt3_this_question.append(answer_gpt3_this_prompt)

            answer_palm2_this_prompt = most_common_answer(answers_palm2_this_prompt)
            answers_palm2_this_question.append(answer_palm2_this_prompt)

            answer_claude2_this_prompt = most_common_answer(answers_claude2_this_prompt)
            answers_claude2_this_question.append(answer_claude2_this_prompt)

            answer_sensenova_this_prompt = most_common_answer(answers_sensenova_this_prompt)
            answers_sensenova_this_question.append(answer_sensenova_this_prompt)

        answers_gpt4.append(answers_gpt4_this_question)
        answers_gpt3.append(answers_gpt3_this_question)
        answers_palm2.append(answers_palm2_this_question)
        answers_claude2.append(answers_claude2_this_question)
        answers_sensenova.append(answers_sensenova_this_question)

    marks_gpt4 = {'marks': [[int(val) for val in (np.array(answer_list) == np.array(correct_answers_list[i])).astype(int)] for i, answer_list in enumerate(answers_gpt4)],
                  'marks_by_vote': [int(val) for val in (np.array([most_common_answer(answer_list) for answer_list in answers_gpt4]) == np.array(correct_answers_list)).astype(int)],
                  'letter_choices': answers_gpt4}

    marks_gpt3 = {'marks': [[int(val) for val in (np.array(answer_list) == np.array(correct_answers_list[i])).astype(int)] for i, answer_list in enumerate(answers_gpt3)],
                  'marks_by_vote': [int(val) for val in (np.array([most_common_answer(answer_list) for answer_list in answers_gpt3]) == np.array(correct_answers_list)).astype(int)],
                  'letter_choices': answers_gpt3}

    marks_palm2 = {'marks': [[int(val) for val in (np.array(answer_list) == np.array(correct_answers_list[i])).astype(int)] for i, answer_list in enumerate(answers_palm2)],
                   'marks_by_vote': [int(val) for val in (np.array([most_common_answer(answer_list) for answer_list in answers_palm2]) == np.array(correct_answers_list)).astype(int)],
                   'letter_choices': answers_palm2}

    marks_claude2 = {'marks': [[int(val) for val in (np.array(answer_list) == np.array(correct_answers_list[i])).astype(int)] for i, answer_list in enumerate(answers_claude2)],
                   'marks_by_vote': [int(val) for val in (np.array([most_common_answer(answer_list) for answer_list in answers_claude2]) == np.array(correct_answers_list)).astype(int)],
                   'letter_choices': answers_claude2}

    marks_sensenova = {'marks': [[int(val) for val in (np.array(answer_list) == np.array(correct_answers_list[i])).astype(int)] for i, answer_list in enumerate(answers_sensenova)],
                   'marks_by_vote': [int(val) for val in (np.array([most_common_answer(answer_list) for answer_list in answers_sensenova]) == np.array(correct_answers_list)).astype(int)],
                   'letter_choices': answers_palm2}

    return {'GPT-4': marks_gpt4, 'GPT-3.5': marks_gpt3, 'PaLM 2': marks_palm2, 'Claude 2': marks_claude2, 'SenseNova': marks_sensenova}


#############
## TESTING ##
#############

# temp=0.0
# gpt_one_shot('You are a helpful assistant', 'Hi', print_response=True, model="gpt-4", retries=5, temperature=temp)
# gpt_one_shot('You are a helpful assistant', 'Hi', print_response=True, model="gpt-3.5-turbo", retries=5, temperature=temp)
# palm_one_shot('You are a helpful assistant', 'Hi', print_response=True, retries=5, temperature=temp)
# claude_one_shot('You are a helpful assistant', 'Hi', print_response=True, retries=5, temperature=temp)
# sensenova_one_shot('You are a helpful assistant', 'Hi', print_response=True, retries=5, temperature=temp)

# print the questions
tests, filenames = process_files('../../../Downloads/')
for filename in filenames:
    print(filename)
    print('\t' + 'Categories: ' + str(tests[filename]['categories']))
    print('\t' + 'Number of questions: ' + str(len(tests[filename]['questions'])))
    print('\t' + 'Number of answers: ' + str(len(tests[filename]['answers'])))
    print('\t' + 'Comments: ' + str(tests[filename]['comments']))
    print('')

# # do the first 2 questions of each test for 2 prompts
# instructions_prompts = [
#                         "Please answer the following practice question as if you were a resident in training for a medical exam. Only give the correct answer in your response, a single letter choice. Do not explain your answer.",
#                         "We want to test your knowledge about the medical field. For this reason, we have created some questions to ask you. In your response, only report the correct answer (one letter) without explanation."
#                        ]
# for filename in filenames:
#     do_test(instructions_prompts, tests[filename]['questions'][0:2], tests[filename]['answers'][0:2], temperature=1.0, samples_per_prompt=1)

# Define the instructions prompts
instructions_prompts = ["I am going to ask a multiple-choice question on a medical topic for the purpose of evaluating your understanding of the medical field.\nFor the following question, provide the correct answer, one letter choice, without any justification.",
                        "I want to evaluate your understanding of a medical topic by asking some multiple choice questions. Please answer the following question, designed for the purpose of evaluating your understanding of the topic. Provide the most correct letter choice (only one letter) without explanation.",
                        "I'm a researcher that is trying to figure out how intelligent are. Answer the following question as best you can. Return the best letter choice with no explanation.",
                        "Please answer the following practice question as if you were a resident in training for a medical exam. Only give the correct answer in your response, a single letter choice. Do not explain your answer.",
                        "We want to test your knowledge about the medical field. For this reason, we have created some questions to ask you. In your response, only report the correct answer (one letter) without explanation."
                       ]

# Read the test files
tests, filenames = process_files('../../../Downloads/llm-tests')

# Save tests dictionary to disk as json file
with open('../../../Downloads/llm-tests/tests.json', 'w') as json_file:
    json.dump(tests, json_file, indent=4)

# Give the tests to the LLMs
marks = {}
for filename in filenames:
    marks[filename] = do_test(instructions_prompts, tests[filename]['questions'], tests[filename]['answers'], temperature=1.0, samples_per_prompt=1)

    # Save the marks to disk as json file
    with open('../../../Downloads/llm-tests/marks.json', 'w') as json_file:
        json.dump(marks, json_file, indent=4)

